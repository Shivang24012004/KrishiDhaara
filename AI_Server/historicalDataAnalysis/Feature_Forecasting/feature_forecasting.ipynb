{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T13:58:32.989866Z",
     "iopub.status.busy": "2025-03-23T13:58:32.989623Z",
     "iopub.status.idle": "2025-03-23T13:58:32.994315Z",
     "shell.execute_reply": "2025-03-23T13:58:32.993467Z",
     "shell.execute_reply.started": "2025-03-23T13:58:32.989846Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib, os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, LayerNormalization, RepeatVector\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T13:58:33.561130Z",
     "iopub.status.busy": "2025-03-23T13:58:33.560831Z",
     "iopub.status.idle": "2025-03-23T13:58:33.568974Z",
     "shell.execute_reply": "2025-03-23T13:58:33.568141Z",
     "shell.execute_reply.started": "2025-03-23T13:58:33.561107Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T13:58:33.817850Z",
     "iopub.status.busy": "2025-03-23T13:58:33.817552Z",
     "iopub.status.idle": "2025-03-23T13:58:33.822620Z",
     "shell.execute_reply": "2025-03-23T13:58:33.821553Z",
     "shell.execute_reply.started": "2025-03-23T13:58:33.817827Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found, using CPU instead\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"GPU is available: {len(gpus)} GPU(s) detected\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU found, using CPU instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T13:58:34.298639Z",
     "iopub.status.busy": "2025-03-23T13:58:34.298354Z",
     "iopub.status.idle": "2025-03-23T13:58:34.304937Z",
     "shell.execute_reply": "2025-03-23T13:58:34.303991Z",
     "shell.execute_reply.started": "2025-03-23T13:58:34.298616Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import Metric\n",
    "import tensorflow as tf\n",
    "\n",
    "class RSquare(Metric):\n",
    "    def __init__(self, name='r_square', **kwargs):\n",
    "        super(RSquare, self).__init__(name=name, **kwargs)\n",
    "        self.total_sum = self.add_weight(name='total_sum', initializer='zeros')\n",
    "        self.residual_sum = self.add_weight(name='residual_sum', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        \n",
    "        # Calculate total sum of squares\n",
    "        mean_y_true = tf.reduce_mean(y_true)\n",
    "        total_error = tf.reduce_sum(tf.square(y_true - mean_y_true))\n",
    "        \n",
    "        # Calculate residual sum of squares\n",
    "        unexplained_error = tf.reduce_sum(tf.square(y_true - y_pred))\n",
    "        \n",
    "        # Update state\n",
    "        self.total_sum.assign_add(total_error)\n",
    "        self.residual_sum.assign_add(unexplained_error)\n",
    "\n",
    "    def result(self):\n",
    "        return 1.0 - tf.math.divide_no_nan(self.residual_sum, self.total_sum)\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.total_sum.assign(0.)\n",
    "        self.residual_sum.assign(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T13:58:34.489897Z",
     "iopub.status.busy": "2025-03-23T13:58:34.489675Z",
     "iopub.status.idle": "2025-03-23T13:58:34.517454Z",
     "shell.execute_reply": "2025-03-23T13:58:34.516736Z",
     "shell.execute_reply.started": "2025-03-23T13:58:34.489878Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class IrrigationTimeSeriesForecaster:\n",
    "    def __init__(self, look_back=18, forecast_horizon=6):\n",
    "        self.look_back = look_back\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.temp_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        self.humidity_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        self.models = {\n",
    "            'temperature': None,\n",
    "            'humidity': None\n",
    "        }\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Load and preprocess the dataset\"\"\"\n",
    "        # Load data\n",
    "        df = pd.read_csv(\"../Dataset/Better_Dataset.csv\")\n",
    "        \n",
    "        # Convert date column to datetime\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        \n",
    "        # Set date as index\n",
    "        df.set_index('date', inplace=True)\n",
    "        \n",
    "        # Check for missing values\n",
    "        if df.isnull().sum().sum() > 0:\n",
    "            print(f\"Found {df.isnull().sum().sum()} missing values. Filling with forward fill method.\")\n",
    "            df = df.ffill()  # Forward fill\n",
    "            # If there are still missing values at the beginning, fill them with backward fill\n",
    "            if df.isnull().sum().sum() > 0:\n",
    "                df = df.bfill()\n",
    "        \n",
    "        # Check for duplicated timestamps\n",
    "        if df.index.duplicated().sum() > 0:\n",
    "            print(f\"Found {df.index.duplicated().sum()} duplicated timestamps. Keeping the first occurrence.\")\n",
    "            df = df[~df.index.duplicated(keep='first')]\n",
    "        \n",
    "        # Sort by date\n",
    "        df = df.sort_index()\n",
    "        \n",
    "        # Check if data is evenly spaced\n",
    "        time_diffs = df.index.to_series().diff().dropna()\n",
    "        if len(time_diffs.unique()) > 1:\n",
    "            print(\"Warning: Time series has irregular intervals. Consider resampling.\")\n",
    "            # Resample to 4-hour intervals\n",
    "            df = df.resample('4H').mean().interpolate(method='time')\n",
    "        \n",
    "        # Extract relevant features\n",
    "        self.df = df[['temperature', 'humidity']]\n",
    "        \n",
    "        # Normalize the data\n",
    "        self.df['temperature_scaled'] = self.temp_scaler.fit_transform(df[['temperature']])\n",
    "        self.df['humidity_scaled'] = self.humidity_scaler.fit_transform(df[['humidity']])\n",
    "        \n",
    "        joblib.dump(self.temp_scaler, \"temp_scaler.pkl\")\n",
    "        joblib.dump(self.humidity_scaler, \"humidity_scaler.pkl\")\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def create_sequences(self, data, target_col):\n",
    "        \"\"\"Create input-output pairs for training the model\"\"\"\n",
    "        X, y = [], []\n",
    "        scaled_col = f\"{target_col}_scaled\"\n",
    "        \n",
    "        for i in range(len(data) - self.look_back - self.forecast_horizon + 1):\n",
    "            # Input sequence (look_back days)\n",
    "            X.append(data[scaled_col].values[i:(i + self.look_back)])\n",
    "            \n",
    "            # Output sequence (next forecast_horizon timestamps)\n",
    "            y.append(data[scaled_col].values[(i + self.look_back):(i + self.look_back + self.forecast_horizon)])\n",
    "        \n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def split_data(self, X, y, train_ratio=0.7, val_ratio=0.15):\n",
    "        \"\"\"Split data into training, validation, and test sets\"\"\"\n",
    "        n = len(X)\n",
    "        train_size = int(n * train_ratio)\n",
    "        val_size = int(n * val_ratio)\n",
    "        \n",
    "        # Training set\n",
    "        X_train, y_train = X[:train_size], y[:train_size]\n",
    "        \n",
    "        # Validation set\n",
    "        X_val, y_val = X[train_size:train_size + val_size], y[train_size:train_size + val_size]\n",
    "        \n",
    "        # Test set\n",
    "        X_test, y_test = X[train_size + val_size:], y[train_size + val_size:]\n",
    "        \n",
    "        return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "    \n",
    "    def build_model(self, input_shape):\n",
    "        from tensorflow.keras.layers import Input, Attention, concatenate\n",
    "        from tensorflow.keras.models import Model\n",
    "        from tensorflow.keras.losses import Huber\n",
    "    \n",
    "        # Input layer\n",
    "        inputs = Input(shape=input_shape)\n",
    "        \n",
    "        # Encoder LSTM\n",
    "        lstm1 = Bidirectional(LSTM(256, return_sequences=True))(inputs)\n",
    "        norm1 = LayerNormalization()(lstm1)\n",
    "        drop1 = Dropout(0.3)(norm1)\n",
    "        \n",
    "        # Context vector\n",
    "        lstm2 = Bidirectional(LSTM(128, return_sequences=False))(drop1)\n",
    "        norm2 = LayerNormalization()(lstm2)\n",
    "        drop2 = Dropout(0.3)(norm2)\n",
    "        \n",
    "        # Repeat vector for sequence output\n",
    "        repeated = RepeatVector(self.forecast_horizon)(drop2)\n",
    "        \n",
    "        # Decoder LSTM with attention\n",
    "        decoder_lstm = LSTM(128, return_sequences=True)(repeated)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        attention = Attention()([decoder_lstm, decoder_lstm])\n",
    "        concat = concatenate([decoder_lstm, attention])\n",
    "        \n",
    "        # Output layers\n",
    "        dense1 = Dense(128, activation='relu')(concat)\n",
    "        outputs = Dense(1)(dense1)\n",
    "    \n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        \n",
    "        # Use Huber loss class instead of string identifier\n",
    "        model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                      loss=Huber(),\n",
    "                      metrics=[RSquare()])\n",
    "        return model\n",
    "    \n",
    "    def train_model(self, target_col, epochs=100, batch_size=512, verbose=1):\n",
    "        \"\"\"Train the forecasting model for a specific target column\"\"\"\n",
    "        # Create sequences\n",
    "        X, y = self.create_sequences(self.df, target_col)\n",
    "        \n",
    "        # Reshape for LSTM [samples, timesteps, features]\n",
    "        X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "        y = y.reshape((y.shape[0], y.shape[1], 1))\n",
    "        \n",
    "        # Split data\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test = self.split_data(X, y)\n",
    "        \n",
    "        # Build model\n",
    "        model = self.build_model((self.look_back, 1))\n",
    "        \n",
    "        # Create callbacks\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001),\n",
    "            ModelCheckpoint(f'best_{target_col}_model.keras', monitor='val_loss', save_best_only=True)\n",
    "        ]\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=callbacks,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "        # Evaluate model\n",
    "        mse = model.evaluate(X_test, y_test, verbose=0)[0]\n",
    "        print(f\"{target_col} Test MSE: {mse}\")\n",
    "        \n",
    "        # Save model\n",
    "        self.models[target_col] = model\n",
    "        \n",
    "        # # Plot training history\n",
    "        # self.plot_training_history(history, target_col)\n",
    "        \n",
    "        # # Plot predictions\n",
    "        # self.plot_predictions(X_test, y_test, model, target_col)\n",
    "        \n",
    "        return model, history\n",
    "    \n",
    "    def plot_training_history(self, history, target_col):\n",
    "        \"\"\"Plot training and validation loss\"\"\"\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title(f'{target_col} Model - Training and Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.savefig(f'{target_col}_training_history.png')\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_predictions(self, X_test, y_test, model, target_col):\n",
    "        \"\"\"Plot test predictions against actual values\"\"\"\n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Select a sample from test set\n",
    "        sample_idx = np.random.randint(0, len(X_test))\n",
    "        \n",
    "        # Inverse transform the predictions and actual values for the sample\n",
    "        if target_col == 'temperature':\n",
    "            scaler = self.temp_scaler\n",
    "        else:\n",
    "            scaler = self.humidity_scaler\n",
    "        \n",
    "        y_pred_sample = y_pred[sample_idx].reshape(-1, 1)\n",
    "        y_test_sample = y_test[sample_idx].reshape(-1, 1)\n",
    "        \n",
    "        y_pred_sample = scaler.inverse_transform(y_pred_sample)\n",
    "        y_test_sample = scaler.inverse_transform(y_test_sample)\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.plot(range(self.forecast_horizon), y_test_sample, 'b-', label='Actual')\n",
    "        plt.plot(range(self.forecast_horizon), y_pred_sample, 'r--', label='Predicted')\n",
    "        plt.title(f'{target_col} - Actual vs Predicted (Sample)')\n",
    "        plt.xlabel('Time Steps (4-hour intervals)')\n",
    "        plt.ylabel(target_col)\n",
    "        plt.legend()\n",
    "        plt.savefig(f'{target_col}_predictions_sample.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(y_test.reshape(-1), y_pred.reshape(-1))\n",
    "        rmse = np.sqrt(mean_squared_error(y_test.reshape(-1), y_pred.reshape(-1)))\n",
    "        \n",
    "        print(f\"{target_col} Test MAE: {mae}\")\n",
    "        print(f\"{target_col} Test RMSE: {rmse}\")\n",
    "    \n",
    "    def forecast_next_day(self, last_sequence):\n",
    "        # Initialize dataframe for forecasts\n",
    "        forecast_df = pd.DataFrame(index=pd.date_range(\n",
    "            start=last_sequence.index[-1] + pd.Timedelta(hours=4), \n",
    "            periods=self.forecast_horizon, \n",
    "            freq='4H'\n",
    "        ))\n",
    "        \n",
    "        # For each target (temperature and humidity)\n",
    "        for target_col in ['temperature', 'humidity']:\n",
    "            # Prepare input data\n",
    "            scaled_col = f\"{target_col}_scaled\"\n",
    "            input_seq = last_sequence[scaled_col].values[-self.look_back:].reshape(1, self.look_back, 1)\n",
    "            \n",
    "            # Make prediction\n",
    "            scaled_pred = self.models[target_col].predict(input_seq)[0]\n",
    "            \n",
    "            # Inverse transform to get actual values\n",
    "            if target_col == 'temperature':\n",
    "                scaler = self.temp_scaler\n",
    "            else:\n",
    "                scaler = self.humidity_scaler\n",
    "                \n",
    "            predictions = scaler.inverse_transform(scaled_pred.reshape(-1, 1)).flatten()\n",
    "            \n",
    "            # Add to forecast dataframe\n",
    "            forecast_df[target_col] = predictions\n",
    "        \n",
    "        return forecast_df\n",
    "    \n",
    "    def save_models(self, path='saved_models'):\n",
    "        \"\"\"Save trained models\"\"\"\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "            \n",
    "        for target, model in self.models.items():\n",
    "            if model is not None:\n",
    "                model.save(f\"{path}/{target}_model.h5\")\n",
    "                print(f\"Saved {target} model to {path}/{target}_model.h5\")\n",
    "    \n",
    "    def load_models(self, path='saved_models'):\n",
    "        \"\"\"Load saved models\"\"\"\n",
    "        for target in self.models.keys():\n",
    "            model_path = f\"{path}/{target}_model.h5\"\n",
    "            if os.path.exists(model_path):\n",
    "                self.models[target] = tf.keras.models.load_model(model_path)\n",
    "                print(f\"Loaded {target} model from {model_path}\")\n",
    "            else:\n",
    "                print(f\"Model file {model_path} not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T13:58:34.527541Z",
     "iopub.status.busy": "2025-03-23T13:58:34.527267Z",
     "iopub.status.idle": "2025-03-23T13:58:56.415717Z",
     "shell.execute_reply": "2025-03-23T13:58:56.414906Z",
     "shell.execute_reply.started": "2025-03-23T13:58:34.527520Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "\n",
      "Training temperature model...\n",
      "Epoch 1/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 956ms/step - loss: 0.2147 - r_square: -18.7366 - val_loss: 0.0168 - val_r_square: -3.5825 - learning_rate: 0.0010\n",
      "Epoch 2/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 704ms/step - loss: 0.0207 - r_square: -0.8773 - val_loss: 0.0128 - val_r_square: -2.4863 - learning_rate: 0.0010\n",
      "Epoch 3/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 746ms/step - loss: 0.0150 - r_square: -0.3573 - val_loss: 0.0170 - val_r_square: -3.6348 - learning_rate: 0.0010\n",
      "Epoch 4/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 847ms/step - loss: 0.0128 - r_square: -0.1576 - val_loss: 0.0248 - val_r_square: -5.7675 - learning_rate: 0.0010\n",
      "Epoch 5/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 665ms/step - loss: 0.0123 - r_square: -0.1119 - val_loss: 0.0130 - val_r_square: -2.5427 - learning_rate: 0.0010\n",
      "Epoch 6/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 660ms/step - loss: 0.0116 - r_square: -0.0523 - val_loss: 0.0259 - val_r_square: -6.0759 - learning_rate: 0.0010\n",
      "Epoch 7/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 751ms/step - loss: 0.0123 - r_square: -0.1112 - val_loss: 0.0166 - val_r_square: -3.5344 - learning_rate: 0.0010\n",
      "Epoch 8/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 768ms/step - loss: 0.0113 - r_square: -0.0247 - val_loss: 0.0159 - val_r_square: -3.3490 - learning_rate: 5.0000e-04\n",
      "Epoch 9/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 806ms/step - loss: 0.0110 - r_square: 0.0024 - val_loss: 0.0197 - val_r_square: -4.3874 - learning_rate: 5.0000e-04\n",
      "Epoch 10/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 759ms/step - loss: 0.0110 - r_square: -0.0015 - val_loss: 0.0195 - val_r_square: -4.3201 - learning_rate: 5.0000e-04\n",
      "Epoch 11/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 680ms/step - loss: 0.0111 - r_square: -0.0029 - val_loss: 0.0165 - val_r_square: -3.5184 - learning_rate: 5.0000e-04\n",
      "Epoch 12/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 694ms/step - loss: 0.0107 - r_square: 0.0336 - val_loss: 0.0161 - val_r_square: -3.4011 - learning_rate: 5.0000e-04\n",
      "temperature Test MSE: 0.009726615622639656\n",
      "\n",
      "Training humidity model...\n",
      "Epoch 1/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 558ms/step - loss: 0.3586 - r_square: -47.9257 - val_loss: 0.0791 - val_r_square: -22.5051 - learning_rate: 0.0010\n",
      "Epoch 2/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 791ms/step - loss: 0.0353 - r_square: -3.1325 - val_loss: 0.0176 - val_r_square: -4.2276 - learning_rate: 0.0010\n",
      "Epoch 3/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 886ms/step - loss: 0.0159 - r_square: -0.8602 - val_loss: 0.0470 - val_r_square: -12.9773 - learning_rate: 0.0010\n",
      "Epoch 4/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 771ms/step - loss: 0.0132 - r_square: -0.5432 - val_loss: 0.0076 - val_r_square: -1.2624 - learning_rate: 0.0010\n",
      "Epoch 5/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 845ms/step - loss: 0.0128 - r_square: -0.4973 - val_loss: 0.0393 - val_r_square: -10.6888 - learning_rate: 0.0010\n",
      "Epoch 6/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 673ms/step - loss: 0.0122 - r_square: -0.4322 - val_loss: 0.0295 - val_r_square: -7.7673 - learning_rate: 0.0010\n",
      "Epoch 7/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 848ms/step - loss: 0.0099 - r_square: -0.1568 - val_loss: 0.0153 - val_r_square: -3.5559 - learning_rate: 0.0010\n",
      "Epoch 8/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 690ms/step - loss: 0.0098 - r_square: -0.1451 - val_loss: 0.0300 - val_r_square: -7.9036 - learning_rate: 0.0010\n",
      "Epoch 9/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 654ms/step - loss: 0.0098 - r_square: -0.1505 - val_loss: 0.0266 - val_r_square: -6.9183 - learning_rate: 0.0010\n",
      "Epoch 10/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 649ms/step - loss: 0.0092 - r_square: -0.0794 - val_loss: 0.0213 - val_r_square: -5.3169 - learning_rate: 5.0000e-04\n",
      "Epoch 11/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 700ms/step - loss: 0.0091 - r_square: -0.0604 - val_loss: 0.0209 - val_r_square: -5.2012 - learning_rate: 5.0000e-04\n",
      "Epoch 12/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 687ms/step - loss: 0.0089 - r_square: -0.0421 - val_loss: 0.0247 - val_r_square: -6.3326 - learning_rate: 5.0000e-04\n",
      "Epoch 13/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 647ms/step - loss: 0.0092 - r_square: -0.0818 - val_loss: 0.0250 - val_r_square: -6.4388 - learning_rate: 5.0000e-04\n",
      "Epoch 14/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 636ms/step - loss: 0.0092 - r_square: -0.0754 - val_loss: 0.0214 - val_r_square: -5.3639 - learning_rate: 5.0000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "humidity Test MSE: 0.007569835055619478\n",
      "Saved temperature model to saved_models/temperature_model.h5\n",
      "Saved humidity model to saved_models/humidity_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bn/z18v0flj0_125bt4hqljj7lh0000gn/T/ipykernel_77238/2899604945.py:237: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  forecast_df = pd.DataFrame(index=pd.date_range(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 550ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 928ms/step\n",
      "\n",
      "Forecast for the next 24 hours (6 readings at 4-hour intervals):\n",
      "                     temperature   humidity\n",
      "2025-03-21 04:00:00    28.502573  55.275482\n",
      "2025-03-21 08:00:00    27.221292  58.577694\n",
      "2025-03-21 12:00:00    26.277782  59.852825\n",
      "2025-03-21 16:00:00    26.288931  60.318851\n",
      "2025-03-21 20:00:00    26.438679  60.494957\n",
      "2025-03-22 00:00:00    26.546877  60.551815\n",
      "\n",
      "Model training and forecasting completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the forecaster\n",
    "forecaster = IrrigationTimeSeriesForecaster(look_back=18, forecast_horizon=6)\n",
    "\n",
    "print(\"Loading and preprocessing data...\")\n",
    "df = forecaster.load_data()\n",
    "    \n",
    "print(\"\\nTraining temperature model...\")\n",
    "forecaster.train_model('temperature', epochs=150)\n",
    "    \n",
    "print(\"\\nTraining humidity model...\")\n",
    "forecaster.train_model('humidity', epochs=150)\n",
    "    \n",
    "forecaster.save_models()\n",
    "    \n",
    "# Generate forecast for the next day\n",
    "last_data = df.iloc[-forecaster.look_back:]\n",
    "forecast = forecaster.forecast_next_day(last_data)\n",
    "    \n",
    "print(\"\\nForecast for the next 24 hours (6 readings at 4-hour intervals):\")\n",
    "print(forecast)\n",
    "    \n",
    "print(\"\\nModel training and forecasting completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6946571,
     "sourceId": 11137117,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
